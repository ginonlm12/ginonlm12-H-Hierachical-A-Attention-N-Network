{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "csv.field_size_limit(1000000)\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 35)\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n",
    "        super(MyDataset, self).__init__()\n",
    "\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"')\n",
    "            for idx, line in enumerate(reader):\n",
    "                text = \"\"\n",
    "                for tx in line[1:]:\n",
    "                    text += tx.lower()\n",
    "                    text += \" \"\n",
    "                label = int(line[0]) - 1\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                                usecols=[0]).values \n",
    "        # ????????\n",
    "        \n",
    "        self.dict = [word[0] for word in self.dict]\n",
    "        # print(self.dict)\n",
    "        self.max_length_sentences = max_length_sentences\n",
    "        self.max_length_word = max_length_word\n",
    "        self.num_classes = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        text = self.texts[index]\n",
    "        document_encode = [\n",
    "            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "            in\n",
    "            sent_tokenize(text=text)]\n",
    "\n",
    "        for sentences in document_encode:\n",
    "            if len(sentences) < self.max_length_word:\n",
    "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
    "                sentences.extend(extended_words)\n",
    "\n",
    "        if len(document_encode) < self.max_length_sentences:\n",
    "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
    "                                  range(self.max_length_sentences - len(document_encode))]\n",
    "            document_encode.extend(extended_sentences)\n",
    "\n",
    "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
    "                          :self.max_length_sentences]\n",
    "\n",
    "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
    "        document_encode += 1\n",
    "\n",
    "        return document_encode.astype(np.int64), label\n",
    "\n",
    "test = MyDataset(data_path=\"data/train.csv\", dict_path=\"glove.6B.50d.txt\")\n",
    "print (test.__getitem__(index=1)[0].shape)\n",
    "print(test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation(y_true, y_prob, list_metrics):\n",
    "    y_pred = np.argmax(y_prob, -1)\n",
    "    output = {}\n",
    "    if 'accuracy' in list_metrics:\n",
    "        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
    "    if 'loss' in list_metrics:\n",
    "        try:\n",
    "            output['loss'] = metrics.log_loss(y_true, y_prob)\n",
    "        except ValueError:\n",
    "            output['loss'] = -1\n",
    "    if 'confusion_matrix' in list_metrics:\n",
    "        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n",
    "    return output\n",
    "\n",
    "def matrix_mul(input, weight, bias=False):\n",
    "    feature_list = []\n",
    "    for feature in input:\n",
    "        feature = torch.mm(feature, weight)\n",
    "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
    "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
    "        feature = torch.tanh(feature).unsqueeze(0)\n",
    "        feature_list.append(feature)\n",
    "\n",
    "    return torch.cat(feature_list, 0).squeeze()\n",
    "\n",
    "def element_wise_mul(input1, input2):\n",
    "\n",
    "    feature_list = []\n",
    "    for feature_1, feature_2 in zip(input1, input2):\n",
    "        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n",
    "        feature = feature_1 * feature_2\n",
    "        feature_list.append(feature.unsqueeze(0))\n",
    "    output = torch.cat(feature_list, 0)\n",
    "\n",
    "    return torch.sum(output, 0).unsqueeze(0)\n",
    "\n",
    "def get_max_lengths(data_path):\n",
    "    word_length_list = []\n",
    "    sent_length_list = []\n",
    "    with open(data_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, quotechar='\"')\n",
    "        for idx, line in enumerate(reader):\n",
    "            text = \"\"\n",
    "            for tx in line[1:]:\n",
    "                text += tx.lower()\n",
    "                text += \" \"\n",
    "            sent_list = sent_tokenize(text)\n",
    "            sent_length_list.append(len(sent_list))\n",
    "\n",
    "            for sent in sent_list:\n",
    "                word_list = word_tokenize(sent)\n",
    "                word_length_list.append(len(word_list))\n",
    "\n",
    "        sorted_word_length = sorted(word_length_list)\n",
    "        sorted_sent_length = sorted(sent_length_list)\n",
    "\n",
    "    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]\n",
    "\n",
    "word, sent = get_max_lengths(\"data/test.csv\")\n",
    "print (word)\n",
    "print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttNet(nn.Module):\n",
    "    def __init__(self, word2vec_path, hidden_size=50):\n",
    "        super(WordAttNet, self).__init__()\n",
    "        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n",
    "        dict_len, embed_size = dict.shape\n",
    "        dict_len += 1\n",
    "        unknown_word = np.zeros((1, embed_size))\n",
    "        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float64))\n",
    "\n",
    "\n",
    "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
    "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "\n",
    "        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "\n",
    "        self.word_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        output = self.lookup(input)\n",
    "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
    "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output,output.permute(1,0))\n",
    "\n",
    "        return output, h_output\n",
    "\n",
    "abc = WordAttNet(word2vec_path=\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentAttNet(\n",
       "  (gru): GRU(100, 50, bidirectional=True)\n",
       "  (fc): Linear(in_features=100, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentAttNet(nn.Module):\n",
    "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n",
    "        super(SentAttNet, self).__init__()\n",
    "\n",
    "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
    "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
    "\n",
    "        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
    "        # self.sent_softmax = nn.Softmax()\n",
    "        # self.fc_softmax = nn.Softmax()\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        self.sent_weight.data.normal_(mean, std)\n",
    "        self.context_weight.data.normal_(mean, std)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "\n",
    "        f_output, h_output = self.gru(input, hidden_state)\n",
    "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
    "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
    "        output = F.softmax(output)\n",
    "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, h_output\n",
    "\n",
    "abc = SentAttNet()\n",
    "abc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAttNet(nn.Module):\n",
    "    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n",
    "                 max_sent_length, max_word_length):\n",
    "        super(HierAttNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.max_sent_length = max_sent_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n",
    "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
    "        self._init_hidden_state()\n",
    "\n",
    "    def _init_hidden_state(self, last_batch_size=None):\n",
    "        if last_batch_size:\n",
    "            batch_size = last_batch_size\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
    "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
    "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output_list = []\n",
    "        input = input.permute(1, 0, 2)\n",
    "        for i in input:\n",
    "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n",
    "            output_list.append(output)\n",
    "        output = torch.cat(output_list, 0)\n",
    "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.num_epoches = 2\n",
    "        self.lr = 0.1\n",
    "        self.momentum = 0.9\n",
    "        self.word_hidden_size = 50\n",
    "        self.sent_hidden_size = 50\n",
    "        self.es_min_delta = 0.0\n",
    "        self.es_patience = 5\n",
    "        self.train_set = \"data/train.csv\"\n",
    "        self.test_set = \"data/test.csv\"\n",
    "        self.test_interval = 1\n",
    "        self.word2vec_path = \"glove.6B.50d.txt\"\n",
    "        self.log_path = \"tensorboard/han_voc\"\n",
    "        self.saved_path = \"trained_models\"\n",
    "        \n",
    "def train(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "    if not os.path.exists(opt.saved_path):\n",
    "        os.makedirs(opt.saved_path)\n",
    "    output_file = open(opt.saved_path + os.sep + \"logs.txt\", \"w\")\n",
    "    output_file.write(\"Model's parameters: {}\".format(vars(opt)))\n",
    "    training_params = {\"batch_size\": opt.batch_size,\n",
    "                       \"shuffle\": True,\n",
    "                       \"drop_last\": True}\n",
    "    test_params = {\"batch_size\": opt.batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "\n",
    "    max_word_length, max_sent_length = get_max_lengths(opt.train_set)\n",
    "    training_set = MyDataset(opt.train_set, opt.word2vec_path, max_sent_length, max_word_length)\n",
    "    training_generator = DataLoader(training_set, **training_params)\n",
    "    test_set = MyDataset(opt.test_set, opt.word2vec_path, max_sent_length, max_word_length)\n",
    "    test_generator = DataLoader(test_set, **test_params)\n",
    "\n",
    "    model = HierAttNet(opt.word_hidden_size, opt.sent_hidden_size, opt.batch_size, training_set.num_classes,\n",
    "                       opt.word2vec_path, max_sent_length, max_word_length)\n",
    "\n",
    "\n",
    "    if os.path.isdir(opt.log_path):\n",
    "        shutil.rmtree(opt.log_path)\n",
    "    os.makedirs(opt.log_path)\n",
    "    writer = SummaryWriter(opt.log_path)\n",
    "    # writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr, momentum=opt.momentum)\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "    for epoch in range(opt.num_epoches):\n",
    "        for iter, (feature, label) in enumerate(training_generator):\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                label = label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            model._init_hidden_state()\n",
    "            predictions = model(feature)\n",
    "            loss = criterion(predictions, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"accuracy\"])\n",
    "            print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                opt.num_epoches,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                loss, training_metrics[\"accuracy\"]))\n",
    "            writer.add_scalar('Train/Loss', loss, epoch * num_iter_per_epoch + iter)\n",
    "            writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
    "        if epoch % opt.test_interval == 0:\n",
    "            model.eval()\n",
    "            loss_ls = []\n",
    "            te_label_ls = []\n",
    "            te_pred_ls = []\n",
    "            for te_feature, te_label in test_generator:\n",
    "                num_sample = len(te_label)\n",
    "                if torch.cuda.is_available():\n",
    "                    te_feature = te_feature.cuda()\n",
    "                    te_label = te_label.cuda()\n",
    "                with torch.no_grad():\n",
    "                    model._init_hidden_state(num_sample)\n",
    "                    te_predictions = model(te_feature)\n",
    "                te_loss = criterion(te_predictions, te_label)\n",
    "                loss_ls.append(te_loss * num_sample)\n",
    "                te_label_ls.extend(te_label.clone().cpu())\n",
    "                te_pred_ls.append(te_predictions.clone().cpu())\n",
    "            te_loss = sum(loss_ls) / test_set.__len__()\n",
    "            te_pred = torch.cat(te_pred_ls, 0)\n",
    "            te_label = np.array(te_label_ls)\n",
    "            test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "            output_file.write(\n",
    "                \"Epoch: {}/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n\".format(\n",
    "                    epoch + 1, opt.num_epoches,\n",
    "                    te_loss,\n",
    "                    test_metrics[\"accuracy\"],\n",
    "                    test_metrics[\"confusion_matrix\"]))\n",
    "            print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
    "                epoch + 1,\n",
    "                opt.num_epoches,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                te_loss, test_metrics[\"accuracy\"]))\n",
    "            writer.add_scalar('Test/Loss', te_loss, epoch)\n",
    "            writer.add_scalar('Test/Accuracy', test_metrics[\"accuracy\"], epoch)\n",
    "            model.train()\n",
    "            if te_loss + opt.es_min_delta < best_loss:\n",
    "                best_loss = te_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model, opt.saved_path + os.sep + \"whole_model_han\")\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch - best_epoch > opt.es_patience > 0:\n",
    "                print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Args()\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    args = {\n",
    "        \"batch_size\": 128,\n",
    "        \"data_path\": \"data/test.csv\",\n",
    "        \"pre_trained_model\": \"trained_models/whole_model_han\",\n",
    "        \"word2vec_path\": \"glove.6B.50d.txt\",\n",
    "        \"output\": \"predictions\"\n",
    "    }\n",
    "    return args\n",
    "\n",
    "def test(opt):\n",
    "    test_params = {\"batch_size\": opt[\"batch_size\"],\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False}\n",
    "    if os.path.isdir(opt[\"output\"]):\n",
    "        shutil.rmtree(opt[\"output\"])\n",
    "    os.makedirs(opt[\"output\"], exist_ok=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.load(opt[\"pre_trained_model\"])\n",
    "    else:\n",
    "        model = torch.load(opt[\"pre_trained_model\"], map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    test_set = MyDataset(opt[\"data_path\"], opt[\"word2vec_path\"], model.max_sent_length, model.max_word_length)\n",
    "    test_generator = DataLoader(test_set, **test_params)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    te_label_ls = []\n",
    "    te_pred_ls = []\n",
    "    \n",
    "    for te_feature, te_label in test_generator:\n",
    "        num_sample = len(te_label)\n",
    "        if torch.cuda.is_available():\n",
    "            te_feature = te_feature.cuda()\n",
    "            te_label = te_label.cuda()\n",
    "        with torch.no_grad():\n",
    "            model._init_hidden_state(num_sample)\n",
    "            te_predictions = model(te_feature)\n",
    "            te_predictions = F.softmax(te_predictions, dim=1)\n",
    "        te_label_ls.extend(te_label.clone().cpu())\n",
    "        te_pred_ls.append(te_predictions.clone().cpu())\n",
    "        \n",
    "    te_pred = torch.cat(te_pred_ls, 0).numpy()\n",
    "    te_label = np.array(te_label_ls)\n",
    "\n",
    "    fieldnames = ['True label', 'Predicted label', 'Content']\n",
    "    with open(os.path.join(opt[\"output\"], \"predictions.csv\"), 'w') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writeheader()\n",
    "        for i, j, k in zip(te_label, te_pred, test_set.texts):\n",
    "            writer.writerow({'True label': i + 1, 'Predicted label': np.argmax(j) + 1, 'Content': k})\n",
    "\n",
    "    test_metrics = get_evaluation(te_label, te_pred, list_metrics=[\"accuracy\", \"loss\", \"confusion_matrix\"])\n",
    "    print(\"Prediction:\\nLoss: {} Accuracy: {} \\nConfusion matrix: \\n{}\".format(\n",
    "        test_metrics[\"loss\"], test_metrics[\"accuracy\"], test_metrics[\"confusion_matrix\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_31580\\2649210167.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(output)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_31580\\1252128706.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(output)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m opt \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[1;32m----> 2\u001b[0m test(opt)\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m     30\u001b[0m te_label_ls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m te_pred_ls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m te_feature, te_label \u001b[38;5;129;01min\u001b[39;00m test_generator:\n\u001b[0;32m     34\u001b[0m     num_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(te_label)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     34\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[index]\n\u001b[1;32m---> 36\u001b[0m document_encode \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict\u001b[38;5;241m.\u001b[39mindex(word) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text\u001b[38;5;241m=\u001b[39msentences)] \u001b[38;5;28;01mfor\u001b[39;00m sentences\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     sent_tokenize(text\u001b[38;5;241m=\u001b[39mtext)]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentences \u001b[38;5;129;01min\u001b[39;00m document_encode:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length_word:\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[index]\n\u001b[0;32m     36\u001b[0m document_encode \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 37\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict\u001b[38;5;241m.\u001b[39mindex(word) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text\u001b[38;5;241m=\u001b[39msentences)] \u001b[38;5;28;01mfor\u001b[39;00m sentences\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     sent_tokenize(text\u001b[38;5;241m=\u001b[39mtext)]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentences \u001b[38;5;129;01min\u001b[39;00m document_encode:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length_word:\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[index]\n\u001b[0;32m     36\u001b[0m document_encode \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 37\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict\u001b[38;5;241m.\u001b[39mindex(word) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text\u001b[38;5;241m=\u001b[39msentences)] \u001b[38;5;28;01mfor\u001b[39;00m sentences\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     sent_tokenize(text\u001b[38;5;241m=\u001b[39mtext)]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentences \u001b[38;5;129;01min\u001b[39;00m document_encode:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length_word:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = get_args()\n",
    "test(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
